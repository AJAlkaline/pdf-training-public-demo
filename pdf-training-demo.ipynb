{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9966335f-f9bf-4ddb-812f-8608396dcfb4",
   "metadata": {},
   "source": [
    "# PDF Training Demo\n",
    "\n",
    "### Dependencies\n",
    "- Python 3.11\n",
    "    - https://www.python.org/downloads/\n",
    "- Poppler for pdf2image\n",
    "    - https://pypi.org/project/pdf2image/\n",
    "\n",
    "### Before Running\n",
    "After installing above dependencies run\n",
    "```\n",
    "python.exe -m pip install --upgrade pip\n",
    "pip install requirements.txt\n",
    "```\n",
    "\n",
    "Then copy `.env.example` and replace placeholders with your correct Google API Key, Google Application Credentials, Google Project Name, and Huggingface Token, and save as `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c71da6-a9ab-4460-826c-6640d3f12403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Takes all pdfs in the directory tree, converts them to images using pdf2image\n",
    "# then uses Google Vertex AI to parse the pdf images for text\n",
    "# builds the text into a structured dataset for LoRA training\n",
    "# then trains a LoRA for unsloth/Mistral-Small-Instruct-2409-bnb-4bit\n",
    "\n",
    "# pip install pdf2image\n",
    "# Poppler: https://github.com/oschwartz10612/poppler-windows/releases/\n",
    "# F:\\repos\\HobieLLM\\PDF Library\\Beekeeping\\10_things_to_help_bees_UK.pdf\n",
    "\n",
    "# import module\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "input_dir_str = input()\n",
    "\n",
    "def get_pdf_paths(input_dir):\n",
    "    for dirpath, _, filenames in os.walk(input_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                yield os.path.join(dirpath, filename)\n",
    "\n",
    "\n",
    "pdf_paths = list(get_pdf_paths(input_dir_str))\n",
    "\n",
    "# Store Pdf with convert_from_path function\n",
    "def convert_and_save_pdf_images(pdf_path): \n",
    "    images = convert_from_path(pdf_path)\n",
    "\n",
    "    pdf_name = os.path.splitext(os.path.split(pdf_path)[1])[0]\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        # Save pages as images in the pdf\n",
    "        images[i].save('pdf_images/' + pdf_name + '-page'+ str(i) +'.jpg', 'JPEG')\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    print(pdf_path)\n",
    "    convert_and_save_pdf_images(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ae702-33a8-43db-9574-ef5cd74249d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query Google Vertex AI to parse image for text\n",
    "# Make text into a structured json dataset for LoRA training\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "import time\n",
    "import base64\n",
    "import vertexai\n",
    "from vertexai.generative_models import HarmBlockThreshold, HarmCategory, GenerativeModel, Part, FinishReason, SafetySetting\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "load_dotenv()\n",
    "vertexai.init(\n",
    "    project=os.environ[\"GOOGLE_PROJECT\"], \n",
    "    credentials=service_account.Credentials.from_service_account_file(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\n",
    ")\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-flash-001\",\n",
    ")\n",
    "safety_config = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    ),\n",
    "]\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "prompt_text = (\"Return all the text in this image. \"\n",
    "               \"Remove any linebreaks that occur in the middle of a sentence. \"\n",
    "               \"Additionally, return any text or data presented in tables or charts in the form of \"\n",
    "               \"comma-separated values starting with the text 'Table {unique_table_title}:'. \"\n",
    "               \"Additionally, describe any non-text pictures or diagrams starting with the text \"\n",
    "               \"'Diagram Description:'.\")\n",
    "\n",
    "pdf_text_data = []\n",
    "\n",
    "def get_image_text(image_path, count=0):\n",
    "    pil_image = Image.open(image_path)\n",
    "    \n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='jpeg')\n",
    "    \n",
    "    blob = Part.from_data(\n",
    "        mime_type='image/jpeg',\n",
    "        data=img_byte_arr.getvalue()\n",
    "    )\n",
    "\n",
    "    responses = model.generate_content(\n",
    "        [prompt_text, blob],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_config,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response_text = []\n",
    "\n",
    "    try: \n",
    "        for response in responses:\n",
    "            # print(response.text, end=\"\")\n",
    "            response_text.append(response.text)\n",
    "            \n",
    "        pdf_text_data.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"text\": \"\".join(response_text)\n",
    "        })\n",
    "    except:\n",
    "        print(\"Couldn't get text from response for \" + image_path)\n",
    "        time.sleep(5)\n",
    "        if count < 1:\n",
    "            get_image_text(image_path, count+1)\n",
    "\n",
    "def get_image_paths(input_dir):\n",
    "    for dirpath, _, filenames in os.walk(input_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith(\".jpg\"):\n",
    "                yield os.path.join(dirpath, filename)\n",
    "\n",
    "for image_path in get_image_paths(\"./pdf_images\"):\n",
    "    time.sleep(2) # wait 10s to not go over api quota\n",
    "    print(image_path)\n",
    "    get_image_text(image_path)\n",
    "    with open('./data/data.json', 'w') as f:\n",
    "        json.dump(pdf_text_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7acbb-3167-4cac-9cdc-69e952029122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LoRA for use with unsloth/Mistral-Small-Instruct-2409-bnb-4bit on the pdf text dataset\n",
    "# Using unsloth FastLanguageModel based on the transformers package\n",
    "\n",
    "from unsloth import FastLanguageModel \n",
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "token = os.environ[\"HUGGINGFACE_TOKEN\"]\n",
    "\n",
    "login(\n",
    "  token=token,\n",
    ")\n",
    "\n",
    "max_seq_length = 2409\n",
    "\n",
    "data_file_name = input()\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_file_name, split=\"train\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Mistral-Small-Instruct-2409-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 256,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 512,\n",
    "    lora_dropout = 0, # 0 is optimized\n",
    "    bias = \"none\",    # \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # \"unsloth\" for very long context\n",
    "    random_state = 2222,\n",
    "    max_seq_length = max_seq_length,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"output\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    tokenizer = tokenizer,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 100,\n",
    "        num_train_epochs=3,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 100,\n",
    "        save_steps = 1000,\n",
    "        learning_rate=2e-4,\n",
    "        output_dir = \"outputs\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 2222,\n",
    "        report_to=\"tensorboard\",\n",
    "    ),\n",
    ")\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "\n",
    " \n",
    "# save model\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
